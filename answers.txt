answers.txt
Group 29
1. Jacek Janczura - jacek.janczura@campus.tu-berlin.de - 404975
2. Igor Molcean - molcean@campus.tu-berlin.de - 398366
3. Michał Zwolak - michal.zwolak@campus.tu-berlin.de - 403970
4. Mikołaj Robakowski - mikolaj.robakowski@campus.tu-berlin.de - 404926

CPU benchmark questions:
1. Look at your LINPACK measurements. How do they differ between the platforms, what are the main reasons for the differences?

Native - Avg. 3387000,94 KFLOPS
KVM    - Avg. 3383373,01 KFLOPS
Qemu   - Avg.   49485,31 KFLOPS
Docker - Avg. 3435431,58 KFLOPS

Benchmarks on native machine, KVM and Docker virtual machines yield similar results. CPU on QEMU without KVM is much
slower. This happens, because emulation of the CPU done by QEMU is software based, thus it applies noticeable overhead.

Memory benchmark questions:
1. Look at your memsweep measurements. How do they differ between the platforms, what are the main reasons for the differences?

Native - Avg.  0,34 s
KVM    - Avg.  0,52 s
Qemu   - Avg. 20,47 s
Docker - Avg.  0,34 s

In case of Docker, direct access to the MMU is performed => no overhead.

In case of KVM, a dedicated hardware unit is used to translate Guest Physical Address into Host Physical Address =>
small overhead.

In case of Qemu, shadow page tables are used. Every page fault leads to a so-called VM exit, i.e. switching the
contexts from the guest to the host => quite expensive.


Disk benchmark questions:
1. Look at your disk write measurements. How do they differ between the platforms, what are the main reasons for the differences?

Native - Avg.  787,73 IOPS
KVM    - Avg. 1038,48 IOPS
Qemu   - Avg.  560,60 IOPS
Docker - Avg.  381,17 IOPS

These results took us by surprise. We suspect that docker performance problems may be related to their overlayfs file
system and the fact that docker image was created for each benchmark run - if a new overlay is created on the first
write, that would incur significant performance overhead.


2. Which disk format did you use for qemu? How do you expect this benchmark to behave differently on other disk formats?
The disk format used was qcow2. The performance should have been higher if raw disk format was used. For other disk formats,
we suspect that the performance would be slightly lower, since qcow2 is qemu's native disk format with the most
features and support.


Fork benchmark questions:
1. Look at your fork sum measurements. How do they differ between the platforms, what are the main reasons for the differences?

Native - Avg.  0,81 s
KVM    - Avg.  1,02 s
Qemu   - Avg. 17,93 s
Docker - Avg.  1,03 s

In case of Qemu, privileged instructions, needed for a fork, have to be trapped and emulated.

Nginx benchmark questions:
1. Look at your nginx measurements. How do they differ between the platforms, what are the main reasons for the differences?

Native - Avg.  0,14 s
KVM    - Avg. 40,19 s
Qemu   - Avg. 64,06 s
Docker - Avg.  0,17 s

As expected native and docker are the fastest. The performance degradation in qemu is easily explained by the choice
of networking backend used - User Networking (SLIRP) https://wiki.qemu.org/Documentation/Networking#User_Networking_.28SLIRP.29
This networking backend doesn't require any root privileges and is easiest to use, but adds a lot of overhead.

2. How are these measurements related to the disk benchmark findings?
In our case the measurements do not seem to be related in any significant way.



